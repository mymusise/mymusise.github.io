<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>用Transformer构建自己的GPT2模型 - A little change</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="mymusise" />
  <meta name="description" content="0. 前言 OpenAI 发表 GPT2 已经过去一年多了，在网络上也看到有很多个实现的版本。近期想找一个别人训练好的中文模型进行Finetune，网上找了一圈发现大部" />

  <meta name="keywords" content="mymusise, blog" />






<meta name="generator" content="Hugo 0.40.1" />


<link rel="canonical" href="https://mymusise.github.io/post/dl/transformer_example/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="icon" href="/favicon.ico" />
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">


<link href="/dist/even.min.css?v=2.6.1" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">

<meta property="og:title" content="用Transformer构建自己的GPT2模型" />
<meta property="og:description" content="0. 前言 OpenAI 发表 GPT2 已经过去一年多了，在网络上也看到有很多个实现的版本。近期想找一个别人训练好的中文模型进行Finetune，网上找了一圈发现大部" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://mymusise.github.io/post/dl/transformer_example/" />



<meta property="article:published_time" content="2020-11-03T19:23:14&#43;08:00"/>

<meta property="article:modified_time" content="2020-11-03T19:23:14&#43;08:00"/>











<meta itemprop="name" content="用Transformer构建自己的GPT2模型">
<meta itemprop="description" content="0. 前言 OpenAI 发表 GPT2 已经过去一年多了，在网络上也看到有很多个实现的版本。近期想找一个别人训练好的中文模型进行Finetune，网上找了一圈发现大部">


<meta itemprop="datePublished" content="2020-11-03T19:23:14&#43;08:00" />
<meta itemprop="dateModified" content="2020-11-03T19:23:14&#43;08:00" />
<meta itemprop="wordCount" content="2207">



<meta itemprop="keywords" content="GPT2,Tensorflow,Transformer," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="用Transformer构建自己的GPT2模型"/>
<meta name="twitter:description" content="0. 前言 OpenAI 发表 GPT2 已经过去一年多了，在网络上也看到有很多个实现的版本。近期想找一个别人训练好的中文模型进行Finetune，网上找了一圈发现大部"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Mymusise</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/post/">
        <li class="mobile-menu-item">列表</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">分类</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Mymusise</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/post/">列表</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">分类</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">用Transformer构建自己的GPT2模型</h1>

      <div class="post-meta">
        <span class="post-time"> 2020-11-03 </span>
        <div class="post-category">
            <a href="/categories/dl/">DL</a>
          </div>
        
      </div>
    </header>

    
    

    
    <div class="post-content">
      

<h1 id="0-前言">0. 前言</h1>

<p><code>OpenAI</code> 发表 <code>GPT2</code> 已经过去一年多了，在网络上也看到有很多个实现的版本。近期想找一个别人训练好的中文模型进行Finetune，网上找了一圈发现大部分都是用Pytorch实现的，虽然Github上已经有几个用TF训练好的模型，但感觉代码写的太复杂，不适合上手，要么就是还是<code>TF1.X</code>版本的。作为TF2.0的少年，之前了解过 Huggingface 团队出了个 Transformer 库，里面也包含了GPT2模型，看了下文档整体调用也很简洁，所以决定用 Transformer 搞一个。</p>

<p>最终实现代码： <a href="https://github.com/mymusise/gpt2-quickly">mymusise/gpt2-quickly</a></p>

<p>想‘坐享其成’的同学可以直接跳到末尾： <a href="#一些例子">Example</a></p>

<h1 id="1-踩坑之旅">1. 踩坑之旅</h1>

<h2 id="1-1-tf的支持">1.1. TF的支持</h2>

<p>🤗 <code>Transformer</code> 默认用的是 <code>Pytorch</code> 的API，而且从文档上可以体现出团队更倾向 <code>Pytorch</code> ，部分API暂时还不支持 <code>TF</code> 版本的，比如 <code>TextDataset</code> 。不过官方给出可以通过改写 <code>Dataset</code> 的<a href="https://github.com/huggingface/transformers/issues/8190"> <code>set_format</code> </a>方法，来实现 <code>TextDataset</code> 或者 <code>LineByLineTextDataset</code> 的功能。</p>

<h2 id="1-2-train-finetune的文档">1.2. Train/Finetune的文档</h2>

<p>如果用keras的API去训练 <code>TFGPT2LMHeadModel</code> ，loss是个坑。看官网其他model的例子，以为直接compile就可以了。</p>

<pre><code class="language-python">    loss = model.compute_loss
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)
    model.compile(optimizer=optimizer, loss=loss)
</code></pre>

<p>结果这样直接报错，实际上model的output维度比label要高，包含了每个layer的输出。</p>

<p>最后通过看源码和翻他们的issue才找到关于loss的定义。</p>

<pre><code class="language-python">    loss = model.compute_loss
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)
    model.compile(optimizer=optimizer, loss=loss=[loss, *[None] * model.config.n_layer])
</code></pre>

<p>如果用 <code>TFTrainer</code> 就不会涉及上面loss的定义问题。但如果你的版本是(3.4.0)（当前只测试了这个版本，其他版本也有可能），可能会直接报找不到Pytorch的bug，这个Bug官方会在下一个版本（&gt;3.4.0）修复。3.5.0目前已经发布。</p>

<!-- ## Tokenizer

不知道为什么，GPT2Tokenizer好像不支持中文的，（补充） -->

<h1 id="2-正文">2. 正文</h1>

<h2 id="2-1-dataset">2.1. Dataset</h2>

<p>作为测试，可以先从 <a href="https://github.com/chinese-poetry/chinese-poetry"> <code>chinese-poetry</code> </a> download 几篇诗词过来。当前项目采用rawtext的形式，对于json格式的数据可能需要转换下格式。转化后的数据例子： <a href="https://github.com/mymusise/gpt2-quickly/blob/main/dataset/test/raw.txt">test/raw.txt</a></p>

<pre><code>
$ head -n 3 dataset/test/raw.txt 
忆秦娥 唐诗：【风淅淅。夜雨连云黑。滴滴。窗外芭蕉灯下客。除非魂梦到乡国。免被关山隔。忆忆。一句枕前争忘得。】
送兄 唐诗：【别路云初起，离亭叶正飞。所嗟人异雁，不作一行归。】
再赠 唐诗：【弄玉有夫皆得道，刘纲兼室尽登仙。君能仔细窥朝露，须逐云车拜洞天。】
</code></pre>

<h2 id="2-2-vocabulary">2.2. Vocabulary</h2>

<p>GPT2用的是BPE算法，官方给出的字典大小为50257，并没有包括中文。而且按照原论文的编码方法，只用到了基础ASCII表上的256个字符，这种方法用来编码中文语料显然是不合理的。而且目前在网上没有找到按这种BPE编码比较权威的中文字典，所以下面的例子用我们就直接用Bert的WordPiece来进行举例了。</p>

<p>如果你只是进行小样本测试，可以通过<a href="https://github.com/huggingface/tokenizers"> <code>huggingface/Tokenizers</code> </a> 构建自己的字典，一般小样本的字典集合大小都在1000左右的范围内，这样可以打打缩小模型维度，方便我们测试。以 <code>BertWordPieceTokenizer</code> 为例：</p>

<pre><code class="language-python">from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer()
tokenizer.train(files=['your raw text file'],
                vocab_size=52_000, min_frequency=5)
tokenizer.save_model('path/to/save/')
</code></pre>

<p>笔者发现，现在大部分开源的中文语言模型中，相对于Google的21128大小的字典，我发现大家一般会选<a href="https://github.com/CLUEbenchmark/CLUEPretrainedModels"> <code>CLUE</code> </a>提供的8021大小的字典。</p>

<h2 id="2-3-tokenizer">2.3. Tokenizer</h2>

<p>Tokenization之前，我们需要对数据进行切片预处理，方法参考了<a href="https://github.com/imcaspar/gpt2-ml">gpt2-ml</a>的预处理过程。我们知道GPT2最大支持的输入文本是1024长度，假设先设定每个sample的大小是64（1024同样道理），以 <code>。？！</code> 标点符号为分界，对文本进行分句。并每个sample加入上一个sample的最后一句。按照这种处理方式，上面三行样例就变成：</p>

<pre><code>
1. 忆秦娥 唐诗：【风淅淅。夜雨连云黑。滴滴。窗外芭蕉灯下客。除非魂梦到乡国。免被关山隔。忆忆。一句枕前争忘得。】[PAD][PAD]...[PAD]
2. 一句枕前争忘得。】\n送兄 唐诗：【别路云初起，离亭叶正飞。所嗟人异雁，不作一行归。】[PAD][PAD]...[PAD]
3. ....

</code></pre>

<p>接下来把切片好的raw text丢给Tokenizer进行编码, 下面拿刚刚的样例举个例子：</p>

<pre><code class="language-python">In [5]: tokenizer = BertTokenizer.from_pretrained('path/you/save/')

In [6]: tokenizer(&quot;忆秦娥 唐诗：【风淅淅。夜雨连云黑。滴滴。窗外芭蕉灯下客。除非魂梦到乡国。免被关山隔。忆忆。一句枕前争忘得。】[PAD][PAD]&quot;, return_attention_mask=False, return_token_type_ids=False)
Out[6]: {'input_ids': [2, 405, 713, 1, 230, 843, 1003, 8, 973, 1, 1, 7, 267, 952, 885, 53, 1, 7, 628, 628, 7, 724, 265, 1, 1, 636, 15, 305, 7, 942, 962, 990, 559, 155, 43, 242, 7, 1, 827, 123, 336, 947, 7, 405, 405, 7, 10, 196, 541, 157, 49, 407, 399, 7, 9, 0, 0, 3]}

</code></pre>

<p>实际一般需要预处理的文本量都很大，都是几个G以上甚至几十个G，如果单进程处理会很长时间，这里提供一种多进程Tokenizer的方法供大家参考：<a href="https://github.com/mymusise/gpt2-quickly/blob/main/predata.py">predata.py</a></p>

<p>这里把数据按照进程数进行均分，并分给每个进程encode，encode好的token转成numpy的数组。博主比较懒，看到 <code>TFRecord</code> 和 <code>TFExample</code> “臃肿”的API就不想用（如果大家知道有什么场景用 <code>TFRecord</code> 更好，麻烦在评论里纠正下博主），所以最后用pickle分别导出到对应的二进制文件文件了，像这样：</p>

<pre><code>
$ ls dataset/train 
data_0.pickle   data_1.pickle  data_2.pickle
</code></pre>

<h2 id="2-4-model-initialization">2.4. Model initialization</h2>

<p>这个没什么好说的， <code>Transformer</code> 都给包装好了，先定义下模型的参数:</p>

<pre><code class="language-python">from transformers import GPT2Config

config = GPT2Config(
    architectures=[&quot;TFGPT2LMHeadModel&quot;],   # pretrain的时候用来预加载模型
    model_type=&quot;TFGPT2LMHeadModel&quot;,        # 定义模型类型，导出给`AutoConfig`用，如果要上传到hub请必填
    tokenizer_class=&quot;BertTokenizer&quot;,       # 定义tokenizer类型，导出给`AutoTokenizer`用，如果要上传到hub请必填
    vocab_size=8021,
    n_positions=1024,
    n_ctx=1024,
    n_embd=768,
    n_layer=6,
    n_head=6,
    pad_token_id=tokenizer.pad_token_id,   # 前面构建的tokenizer的 PAD ID
    task_specific_params={
        &quot;text-generation&quot;: {
            &quot;do_sample&quot;: True,
            &quot;max_length&quot;: 120
        }
    }
)
</code></pre>

<p>然后构建模型, 直接把上面定义好的 <code>configs</code> 丢给 <code>TFGPT2LMHeadModel</code> 就创建好了。如果要通过 <code>Keras</code> 的API进行训练的话，需要对模型进行compile一下，前面也提到loss这里会有坑。</p>

<pre><code class="language-python">from transformers import TFGPT2LMHeadModel

model = TFGPT2LMHeadModel(config)
loss = model.compute_loss
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)

model.compile(
    optimizer=optimizer,
    loss=[loss, *[None] * model.config.n_layer],
)
</code></pre>

<h2 id="2-5-train">2.5. Train</h2>

<p>训练前可以自定义个callback，每个epochs结束后保存下模型</p>

<pre><code class="language-python">class AutoSaveCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        self.model.save_pretrained(&quot;path/to/save&quot;)

callbacks = [AutoSaveCallback()]

model.fit(
    train_dataset,
    epochs=50,
    steps_per_epoch=2000,
    callbacks=callbacks,
)
</code></pre>

<h2 id="一些例子">一些例子</h2>

<ul>
<li>你可以在colab上尝试整个训练过程: <a href="https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/gpt2_quickly.ipynb">gpt2_quickly.ipynb</a></li>
<li>一个还在测试中的mediun量级的GPT2中文模型: <a href="https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/gpt2_medium_chinese.ipynb">gpt2_medium_chinese.ipynb</a></li>
<li>基于上面的模型，Finetune的小说生成模型: <a href="https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/ai_noval_demo.ipynb">ai_noval_demo.ipynb</a></li>
</ul>

<div style="text-align:center">
    <img src ="/images/dl/transformer_example/gpt2-medium-chinese-homepage.jpeg" style="width:80%"/>
    <div><a>gpt2_medium_chinese</a></div>
</div>

    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">文章作者</span>
    <span class="item-content">mymusise</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">上次更新</span>
    <span class="item-content">2020-11-03</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">许可协议</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank">CC BY-NC-ND 4.0</a></span>
  </p>
</div>

    
    
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">赞赏支持</label>
  <div class="qr-code">
    
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/images/wechat_pay.png">
        <span>微信打赏</span>
      </label>
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/images/alipay.jpg">
        <span>支付宝打赏</span>
      </label>
  </div>
</div>

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/gpt2/">GPT2</a>
          
          <a href="/tags/tensorflow/">Tensorflow</a>
          
          <a href="/tags/transformer/">Transformer</a>
          
        </div>

      
      <nav class="post-nav">
        
        
          <a class="next" href="/post/dl/other_side_deep_network/">
            <span class="next-text nav-default">从另一个角度看深度神经网络</span>
            <span class="prev-text nav-mobile">下一篇</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
      </div>  
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:mymusise1@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/mymusise" class="iconfont icon-github" title="github"></a>
  <a href="https://mymusise.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    由 <a class="hexo-link" href="https://gohugo.io">Hugo</a> 强力驱动
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    主题 - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <span class="copyright-year">
    &copy; 
    
      2017 - 
    2021
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">mymusise</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js"></script>
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>
<script type="text/javascript" src="/dist/even.min.js?v=2.6.1"></script>
  <script type="text/javascript">
    window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  <script async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML'></script>




</body>
</html>
