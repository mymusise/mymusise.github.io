---
title: "从另一个角度看深度神经网络"
date: 2019-02-03T19:23:14+08:00
draft: true
tags: ["Deep Learning", "Discuss"]
categories: ["DL"]

comment: false
toc: false
# reward: false
mathjax: true
expirydate: 2018-04-06
---

# 0x00 你的网络怎么处理问题的
最开始尝试用DL去做NLP相关任务时候，时不时会想要怎么解析训练出来的网络是怎么去执行这些任务呢？看过NG老师课程的应该都知道`CNN`在图片的边缘识别上是怎么处理的，这样就比较容易理解在像`VGG`这样的深度网络中必定有那么几层是表示着要物体的边缘。但是对于NLP任务，要怎么解析`CNN`的处理过程，怎么可以确定深度网络中就会训练出类似于词性，命名实体等特征？

后来去年刚开始用DL做量化的时候，有个搞金融的朋友问我“你用DL训练出来的模型是怎么做策略的？” Emmm，说实话当时不知道怎么解析好，就只是从概率统计的角度上说了下。然后他问我都用了些什么指标，我说没有，只用了K线和买卖量的数据。后来我在想，训练过的网络中应该会有像EMA这样指标。为了验证这样的想法，下面就开始讨论下。


# 0x01 从最简单的XOR开始
假设有这么个问题，现在有一批用户数据，如果用户是IOS用户，或者已经充值过的，都被划分为优质用户。定义$x_1$表示是否IOS，$x_2$表示是否充值过，取值为`[1, -1]`. 

一般情况对于这样简单的划分，可以直接写一段code来实现这样的功能了。
```
if x1 == 1 or x2 == 1:
    return True
else:
    return False
```

如果把他作为一个线性划分问题来解决, 令决策边界`y = WX + b`, 输出`H(x) = hardlims(y) = hardlims(WX + b)`. 这里直接取 W = $\begin{bmatrix} 1 & 1 \end{bmatrix}$, b = $-1$
$$H(x) = hardlims(
    \begin{bmatrix} 1 & 1 \end{bmatrix}
    \begin{bmatrix} x_1 \\\\ x_2 \end{bmatrix}
    - 1)$$

<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example1.jpeg" style="width:60%"/>
    <div><a>图1</a></div>
</div>

这里可以试下反过来想，对于式子$hardlims(
    \begin{bmatrix} 1 & 1 \end{bmatrix}
    \begin{bmatrix} x_1 \\\\ x_2 \end{bmatrix}
    - 1)$ 的理解，就是如果$x_1$等于1或者$x_2$等于1，就是A类，否则就是B类。

上面的例子用用线性划分来解决似乎有点多此一举，但是实际当中需要处理的问题都比较复杂。比如实际上要真正评定用户的消费能力，除了跟用户渠道，购买力有关系，还会与活跃度，用户的交际圈，甚至是性别年龄等有关系。

因此实际问题中，输入$X\begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ ... \\ x_n \end{pmatrix}$的维度一般都会比较高，如果都用code来实现这些一条条的规则，要写好多好多code不说，维护更是困难，重点是很难写出综合最优的结果。但是用机器学习来处理就可以很好地解决这些问题。


# 0x02 用神经网络拟合需要的特征

再举个Quant里面的例子，传统做法是挑一些因子MA，ADR，BIAS等，把因子结合起来判断买入卖出信号。手动把这些因子写成规则来做量化，并不是个明智直选，当然很多经验老道的Quant大牛可以熟用某些因子来做交易，但是对于新手或者刚入门的来说最好就是机器代替我们去学习这些规则了。

### 神经网络与特征
假设现在输入层是过去n天的交易日K线数据（OHLC），现在试图用BP网络来学习这些规则来。先看一层隐藏层的BP网络结构：
<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example2_1.png"/>
    <div><a>图2-1</a></div>
</div>

我们希望神经网络中在训练的时候能够自动学习出与MA，RSI，BIAS等因子类似的指标，假设如果网络可以实现这样的训练，假设H1学习到了MA因子来，那么$W_1$会是下图这样

> 因子公式：

> $MA = \frac{\sum_{1}^{n}Ci}{n}$

> $RSI = \frac{N天内涨的天数}{N}$

> $BIAS = \frac{当日收盘价 − N日内移动平均收盘价}{N日内移动平均收盘价} $

<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example2_2.png"/>
    <div><a>图2-2</a></div>
</div>

输入$X_j$维度是4*1，权重W_ij的维度是1*4，MA因子就是所有天数的收盘价格的求和平均数，因此就在权重$W_ij$里让 $收盘价格×\frac{1}{n}$, 其他因子都*0 忽略掉。就有 $W_1 X = MA$

对于MA这样的线性因子可以用BP网络构造出来，但是对于`RSI`这类的因子，BP网络就显得有点无能为力了。因为判断涨跌需要用 今天的收盘价$C_t$ - 昨天的收盘价$C_t-1$，这种时序相关的运算BP实现起来有点有心无力。


## 多层神经网络拟合出来的因子

在输入特征不变的情况下，如果想强行在BP网络中拟合出`RSI`因子也不是不可以，下面是一种比较蠢的方式，通过改造网络的结构：

<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example2_3.png"/>
    <div><a>图2-3</a></div>
</div>

通过耗用一个隐藏层中`N-1`个单元来计算的到每天的涨跌结果 $H(n) = hardlim({C}\_{n}-C\_{n-1})$ ，然后通过涨跌结果的到`RSI`。这里为了的到`RSI`这一个因子增加了$4(n-1)^2 + 3(n-1)$个参数，显然是不划算的。

但是如果在输入层每天的K线数据中加入是否涨了$R\_{i}$ 作为特征，
$X_i= \begin{bmatrix} O_i \\  H_i \\  L_i \\ C_i \\ R_i \end{bmatrix}^T$
, 这样通过一个$W_i$就可以变相实现时序运算了

$$ 
\sum_{1}^{n} W_i X_i = \sum\_{1}^{n} \begin{bmatrix} 0 & 0 & 0 & 0 & \frac{1}{n} \end{bmatrix}
        \begin{bmatrix} O_n \\\\ H_n \\\\ L_n \\\\ C_i \\\\ R_i \end{bmatrix}
    = \sum\_{1}^{n} (R_i) = RSI
$$

这里只是用了 5*n 个参数，所以如果增加有用的输入特征，会大大降低训练的成本。

如果企图用一个很深的网络去拟合这些特征，应该要提前考虑下实际任务可能需要网络拟合出什么样的特征来完成任务，对于需要的特征，这样的网络是否有几率拟合出来，当然这些都需要对训练任务有一定的深入的认识才能发现。否则这个很深的网络可能什么都学习不出来或者这样的学习会花很大的成本。

就像上面几个例子中，OHLC这四个特征也只用到了一个，如果希望网络拟合出来的因子跟OHL毫无关系，就可以去掉了。但是实际上会希望网络拟合出更多的因子比如`OBV`，而且计算`OBV`还需要加上当天的交易量`V`作为特征。



## 根据目的，指导神经网络的设计，多少层，维度大概多高，以及什么样的结构，所以会有CNN，RNN，Attention等网络的出现

上一个例子里当天的涨跌结果 $y=f(W\_{t}X_t+W\_{t-1}X\_{t-1})$ ，如果把这个当成一阶因子，那么像上面提及到的`RSI`就是二阶因子。

下面再看一个例子，在Quant里，如果出现: "今天是涨的，昨天也是涨的，但是前天是跌的，以及更前面的三天都是跌"，就认为出现谷底了。下面用这个网络来实现

<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example3_1.png"/>
    <div><a>图3-1</a></div>
</div>

<!-- 可能会想为什么不直接在第二层隐藏层直接计算所有需要的天数，如果结果顺序无关，是可以一层直接计算出来。但是任务是与顺序相关的，而且每判断一次下个状态都需要激活函数来判断，所以BP网络实现这样的逻辑至少还需要三层隐藏层。 -->

上面这个网络最后的确可以找出结果来，显然这是个错误示范，对于找到“涨涨跌跌”这样的运算，本质上是把一阶的`涨/跌`结果做了一次线性运算，所以后面三个隐藏层的内容,实际上都在一个空间里，“涨涨跌跌”也是个二阶因子。最后问题是可以通过一次计算的到:

$$ y = hardlim(UX + b) = hardlim( \begin{bmatrix} 1 & 1 & -1 & -1 \end{bmatrix} \begin{bmatrix} r_1 \\\\ r_2 \\\\ r_3 \\\\ r_4 \end{bmatrix} - 1.9)$$ 

$r_i$取值为[1, -1],`1`代表涨`-1`代表跌,`U`是期望的顺序。


<div style="text-align:center">
    <img src ="/images/dl/other_side_deep_netword/example3_2.png"/>
    <div><a>图3-2</a></div>
</div>

在设计神经网络时候，也并非越深越好，如果需要的特征在前几层就能拟合到，层数越多只会增加训练成本。当然很多情况下我们并不清楚需要些什么特征，而是希望企图通过一个很深的网络拟合出所需要的特征来。


对于时序任务，现在现在常用的都是RNN。最早期RNN被提出来之时，主要用来解决在处理时序数据的时像`图3-1`这些臃肿结构，甚至可以折叠上千层原来结构的网络。

<!-- 那么这时用到的运算就变成了 $y_i=f(W_iXi + Uy\_{i-1})$，假设计算涉及到时序长度为M，则需要M-1层隐藏层。 -->


## 怎么样才更容易拟合出这些有用的特征



### 关于Gradient和局部最优问题



## 用Gradient来说明网络并不一定有效，最优点被局部给包围，mini batch有助跳出局部，遇到平坦区。learning rate 的取值的合适范围



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
});
</script>
