---
title: "ç”¨Transformeræ„å»ºè‡ªå·±çš„GPT2æ¨¡å‹"
date: 2020-11-03T19:23:14+08:00
draft: true
tags: ["GPT2", "Tensorflow", "Transformer"]
categories: ["DL"]

comment: false
toc: false

# reward: false

mathjax: true

# expirydate: 2019-09-06

---

# å‰è¨€

`OpenAI` å‘è¡¨ `GPT2` å·²ç»è¿‡å»ä¸€å¹´å¤šäº†ï¼Œåœ¨ç½‘ç»œä¸Šä¹Ÿçœ‹åˆ°æœ‰å¾ˆå¤šä¸ªå®ç°çš„ç‰ˆæœ¬ã€‚è¿‘æœŸæƒ³æ‰¾ä¸€ä¸ªåˆ«äººè®­ç»ƒå¥½çš„ä¸­æ–‡æ¨¡å‹è¿›è¡ŒFinetuneï¼Œç½‘ä¸Šæ‰¾äº†ä¸€åœˆå‘ç°å¤§éƒ¨åˆ†éƒ½æ˜¯ç”¨Pytorchå®ç°çš„ï¼Œè™½ç„¶Githubä¸Šå·²ç»æœ‰å‡ ä¸ªç”¨TFè®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œä½†æ„Ÿè§‰ä»£ç å†™çš„å¤ªå¤æ‚ï¼Œä¸é€‚åˆä¸Šæ‰‹ï¼Œè¦ä¹ˆå°±æ˜¯è¿˜æ˜¯`TF1.X`ç‰ˆæœ¬çš„ã€‚ä½œä¸ºTF2.0çš„å°‘å¹´ï¼Œä¹‹å‰äº†è§£è¿‡ Huggingface å›¢é˜Ÿå‡ºäº†ä¸ª Transformer åº“ï¼Œé‡Œé¢ä¹ŸåŒ…å«äº†GPT2æ¨¡å‹ï¼Œçœ‹äº†ä¸‹æ–‡æ¡£æ•´ä½“è°ƒç”¨ä¹Ÿå¾ˆç®€æ´ï¼Œæ‰€ä»¥å†³å®šç”¨ Transformer æä¸€ä¸ªã€‚

æœ€ç»ˆå®ç°ä»£ç ï¼š [mymusise/gpt2-quickly](https://github.com/mymusise/gpt2-quickly)

# è¸©å‘ä¹‹æ—…

## - TFçš„æ”¯æŒ

ğŸ¤— `Transformer` é»˜è®¤ç”¨çš„æ˜¯ `Pytorch` çš„APIï¼Œè€Œä¸”ä»æ–‡æ¡£ä¸Šå¯ä»¥ä½“ç°å‡ºå›¢é˜Ÿæ›´å€¾å‘ `Pytorch` ï¼Œéƒ¨åˆ†APIæš‚æ—¶è¿˜ä¸æ”¯æŒ `TF` ç‰ˆæœ¬çš„ï¼Œæ¯”å¦‚ `TextDataset` ã€‚ä¸è¿‡å®˜æ–¹ç»™å‡ºå¯ä»¥é€šè¿‡æ”¹å†™ `Dataset` çš„[ `set_format` ](https://github.com/huggingface/transformers/issues/8190)æ–¹æ³•ï¼Œæ¥å®ç° `TextDataset` æˆ–è€… `LineByLineTextDataset` çš„åŠŸèƒ½ã€‚

## - Train/Finetuneçš„æ–‡æ¡£

å¦‚æœç”¨kerasçš„APIå»è®­ç»ƒ `TFGPT2LMHeadModel` ï¼Œlossæ˜¯ä¸ªå‘ã€‚çœ‹å®˜ç½‘å…¶ä»–modelçš„ä¾‹å­ï¼Œä»¥ä¸ºç›´æ¥compileå°±å¯ä»¥äº†ã€‚

``` python
    loss = model.compute_loss
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)
    model.compile(optimizer=optimizer, loss=loss)
```

ç»“æœè¿™æ ·ç›´æ¥æŠ¥é”™ï¼Œå®é™…ä¸Šmodelçš„outputç»´åº¦æ¯”labelè¦é«˜ï¼ŒåŒ…å«äº†æ¯ä¸ªlayerçš„è¾“å‡ºã€‚

æœ€åé€šè¿‡çœ‹æºç å’Œç¿»ä»–ä»¬çš„issueæ‰æ‰¾åˆ°å…³äºlossçš„å®šä¹‰ã€‚

``` python
    loss = model.compute_loss
    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)
    model.compile(optimizer=optimizer, loss=loss=[loss, *[None] * model.config.n_layer])
```

å¦‚æœç”¨ `TFTrainer` å°±ä¸ä¼šæ¶‰åŠä¸Šé¢lossçš„å®šä¹‰é—®é¢˜ã€‚ä½†å¦‚æœä½ çš„ç‰ˆæœ¬æ˜¯(3.4.0)ï¼ˆå½“å‰åªæµ‹è¯•äº†è¿™ä¸ªç‰ˆæœ¬ï¼Œå…¶ä»–ç‰ˆæœ¬ä¹Ÿæœ‰å¯èƒ½ï¼‰ï¼Œå¯èƒ½ä¼šç›´æ¥æŠ¥æ‰¾ä¸åˆ°Pytorchçš„bugï¼Œè¿™ä¸ªBugå®˜æ–¹ä¼šåœ¨ä¸‹ä¸€ä¸ªç‰ˆæœ¬ï¼ˆ>3.4.0ï¼‰ä¿®å¤ã€‚3.5.0ç›®å‰å·²ç»å‘å¸ƒã€‚

<!-- ## Tokenizer

ä¸çŸ¥é“ä¸ºä»€ä¹ˆï¼ŒGPT2Tokenizerå¥½åƒä¸æ”¯æŒä¸­æ–‡çš„ï¼Œï¼ˆè¡¥å……ï¼‰ -->

# æ­£æ–‡

## æ•°æ®é›†

ä½œä¸ºæµ‹è¯•ï¼Œå¯ä»¥å…ˆä» [ `chinese-poetry` ](https://github.com/chinese-poetry/chinese-poetry) download å‡ ç¯‡è¯—è¯è¿‡æ¥ã€‚å½“å‰é¡¹ç›®é‡‡ç”¨rawtextçš„å½¢å¼ï¼Œå¯¹äºjsonæ ¼å¼çš„æ•°æ®å¯èƒ½éœ€è¦è½¬æ¢ä¸‹æ ¼å¼ã€‚è½¬åŒ–åçš„æ•°æ®ä¾‹å­ï¼š [test/raw.txt](https://github.com/mymusise/gpt2-quickly/blob/main/dataset/test/raw.txt)

``` 

$ head -n 3 dataset/test/raw.txt 
å¿†ç§¦å¨¥ å”è¯—ï¼šã€é£æ·…æ·…ã€‚å¤œé›¨è¿äº‘é»‘ã€‚æ»´æ»´ã€‚çª—å¤–èŠ­è•‰ç¯ä¸‹å®¢ã€‚é™¤éé­‚æ¢¦åˆ°ä¹¡å›½ã€‚å…è¢«å…³å±±éš”ã€‚å¿†å¿†ã€‚ä¸€å¥æ•å‰äº‰å¿˜å¾—ã€‚ã€‘
é€å…„ å”è¯—ï¼šã€åˆ«è·¯äº‘åˆèµ·ï¼Œç¦»äº­å¶æ­£é£ã€‚æ‰€å—Ÿäººå¼‚é›ï¼Œä¸ä½œä¸€è¡Œå½’ã€‚ã€‘
å†èµ  å”è¯—ï¼šã€å¼„ç‰æœ‰å¤«çš†å¾—é“ï¼Œåˆ˜çº²å…¼å®¤å°½ç™»ä»™ã€‚å›èƒ½ä»”ç»†çª¥æœéœ²ï¼Œé¡»é€äº‘è½¦æ‹œæ´å¤©ã€‚ã€‘
```

## Vocabulary

GPT2å®˜æ–¹ç»™å‡ºçš„å­—å…¸å¤§å°ä¸º50257ï¼Œå¦‚æœåªæ˜¯è¿›è¡Œå°æ ·æœ¬æµ‹è¯•ï¼Œå¯ä»¥é€šè¿‡[ `huggingface/Tokenizers` ](https://github.com/huggingface/tokenizers) æ„å»ºè‡ªå·±çš„å­—å…¸ï¼Œä¸€èˆ¬å°æ ·æœ¬çš„å­—å…¸é›†åˆå¤§å°éƒ½åœ¨1000å·¦å³çš„èŒƒå›´å†…ï¼Œè¿™æ ·å¯ä»¥æ‰“æ‰“ç¼©å°æ¨¡å‹ç»´åº¦ï¼Œæ–¹ä¾¿æˆ‘ä»¬æµ‹è¯•ã€‚ä»¥ `BertWordPieceTokenizer` ä¸ºä¾‹ï¼š

``` python
from tokenizers import BertWordPieceTokenizer

tokenizer = BertWordPieceTokenizer()
tokenizer.train(files=['your raw text file'],
                vocab_size=52_000, min_frequency=5)
tokenizer.save_model('path/to/save/')
```

å®é™…ä¸Šï¼Œç°åœ¨å¤§éƒ¨åˆ†ä¸­æ–‡è¯­è¨€æ¨¡å‹ï¼Œç›¸å¯¹äºGoogleçš„21128å¤§å°çš„å­—å…¸ï¼Œæˆ‘å‘ç°å¤§å®¶ä¸€èˆ¬ä¼šé€‰[ `CLUE` ](https://github.com/CLUEbenchmark/CLUEPretrainedModels)æä¾›çš„8021å¤§å°çš„å­—å…¸ã€‚

## Tokenizer

Tokenizationä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å¯¹æ•°æ®è¿›è¡Œåˆ‡ç‰‡é¢„å¤„ç†ï¼Œæ–¹æ³•å‚è€ƒäº†[gpt2-ml](https://github.com/imcaspar/gpt2-ml)çš„é¢„å¤„ç†è¿‡ç¨‹ã€‚æˆ‘ä»¬çŸ¥é“GPT2æœ€å¤§æ”¯æŒçš„è¾“å…¥æ–‡æœ¬æ˜¯1024é•¿åº¦ï¼Œå‡è®¾å…ˆè®¾å®šæ¯ä¸ªsampleçš„å¤§å°æ˜¯64ï¼ˆ1024åŒæ ·é“ç†ï¼‰ï¼Œä»¥ `ã€‚ï¼Ÿï¼` æ ‡ç‚¹ç¬¦å·ä¸ºåˆ†ç•Œï¼Œå¯¹æ–‡æœ¬è¿›è¡Œåˆ†å¥ã€‚å¹¶æ¯ä¸ªsampleåŠ å…¥ä¸Šä¸€ä¸ªsampleçš„æœ€åä¸€å¥ã€‚æŒ‰ç…§è¿™ç§å¤„ç†æ–¹å¼ï¼Œä¸Šé¢ä¸‰è¡Œæ ·ä¾‹å°±å˜æˆï¼š

``` 

1. å¿†ç§¦å¨¥ å”è¯—ï¼šã€é£æ·…æ·…ã€‚å¤œé›¨è¿äº‘é»‘ã€‚æ»´æ»´ã€‚çª—å¤–èŠ­è•‰ç¯ä¸‹å®¢ã€‚é™¤éé­‚æ¢¦åˆ°ä¹¡å›½ã€‚å…è¢«å…³å±±éš”ã€‚å¿†å¿†ã€‚ä¸€å¥æ•å‰äº‰å¿˜å¾—ã€‚ã€‘[PAD][PAD]...[PAD]
2. ä¸€å¥æ•å‰äº‰å¿˜å¾—ã€‚ã€‘\né€å…„ å”è¯—ï¼šã€åˆ«è·¯äº‘åˆèµ·ï¼Œç¦»äº­å¶æ­£é£ã€‚æ‰€å—Ÿäººå¼‚é›ï¼Œä¸ä½œä¸€è¡Œå½’ã€‚ã€‘[PAD][PAD]...[PAD]
3. ....

```

æ¥ä¸‹æ¥æŠŠåˆ‡ç‰‡å¥½çš„raw textä¸¢ç»™Tokenizerè¿›è¡Œç¼–ç , ä¸‹é¢æ‹¿åˆšåˆšçš„æ ·ä¾‹ä¸¾ä¸ªä¾‹å­ï¼š

``` python
In [5]: tokenizer = BertTokenizer.from_pretrained('path/you/save/')

In [6]: tokenizer("å¿†ç§¦å¨¥ å”è¯—ï¼šã€é£æ·…æ·…ã€‚å¤œé›¨è¿äº‘é»‘ã€‚æ»´æ»´ã€‚çª—å¤–èŠ­è•‰ç¯ä¸‹å®¢ã€‚é™¤éé­‚æ¢¦åˆ°ä¹¡å›½ã€‚å…è¢«å…³å±±éš”ã€‚å¿†å¿†ã€‚ä¸€å¥æ•å‰äº‰å¿˜å¾—ã€‚ã€‘[PAD][PAD]", return_attention_mask=False, return_token_type_ids=False)
Out[6]: {'input_ids': [2, 405, 713, 1, 230, 843, 1003, 8, 973, 1, 1, 7, 267, 952, 885, 53, 1, 7, 628, 628, 7, 724, 265, 1, 1, 636, 15, 305, 7, 942, 962, 990, 559, 155, 43, 242, 7, 1, 827, 123, 336, 947, 7, 405, 405, 7, 10, 196, 541, 157, 49, 407, 399, 7, 9, 0, 0, 3]}

```

å®é™…ä¸€èˆ¬éœ€è¦é¢„å¤„ç†çš„æ–‡æœ¬é‡éƒ½å¾ˆå¤§ï¼Œéƒ½æ˜¯å‡ ä¸ªGä»¥ä¸Šç”šè‡³å‡ åä¸ªGï¼Œå¦‚æœå•è¿›ç¨‹å¤„ç†ä¼šå¾ˆé•¿æ—¶é—´ï¼Œè¿™é‡Œæä¾›ä¸€ç§å¤šè¿›ç¨‹Tokenizerçš„æ–¹æ³•ä¾›å¤§å®¶å‚è€ƒï¼š[predata.py](https://github.com/mymusise/gpt2-quickly/blob/main/predata.py) 

è¿™é‡ŒæŠŠæ•°æ®æŒ‰ç…§è¿›ç¨‹æ•°è¿›è¡Œå‡åˆ†ï¼Œå¹¶åˆ†ç»™æ¯ä¸ªè¿›ç¨‹encodeï¼Œencodeå¥½çš„tokenè½¬æˆnumpyçš„æ•°ç»„ã€‚åšä¸»æ¯”è¾ƒæ‡’ï¼Œçœ‹åˆ° `TFRecord` å’Œ `TFExample` â€œè‡ƒè‚¿â€çš„APIå°±ä¸æƒ³ç”¨ï¼ˆå¦‚æœå¤§å®¶çŸ¥é“æœ‰ä»€ä¹ˆåœºæ™¯ç”¨ `TFRecord` æ›´å¥½ï¼Œéº»çƒ¦åœ¨è¯„è®ºé‡Œçº æ­£ä¸‹åšä¸»ï¼‰ï¼Œæ‰€ä»¥æœ€åç”¨pickleåˆ†åˆ«å¯¼å‡ºåˆ°å¯¹åº”çš„äºŒè¿›åˆ¶æ–‡ä»¶æ–‡ä»¶äº†ï¼Œåƒè¿™æ ·ï¼š

``` 

$ ls dataset/train 
data_0.pickle   data_1.pickle  data_2.pickle
```

## Model initialization

è¿™ä¸ªæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œ `Transformer` éƒ½ç»™åŒ…è£…å¥½äº†ï¼Œå…ˆå®šä¹‰ä¸‹æ¨¡å‹çš„å‚æ•°:

``` python
from transformers import GPT2Config

config = GPT2Config(
    architectures=["TFGPT2LMHeadModel"],   # pretrainçš„æ—¶å€™ç”¨æ¥é¢„åŠ è½½æ¨¡å‹
    model_type="TFGPT2LMHeadModel",        # å®šä¹‰æ¨¡å‹ç±»å‹ï¼Œå¯¼å‡ºç»™`AutoConfig`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
    tokenizer_class="BertTokenizer",       # å®šä¹‰tokenizerç±»å‹ï¼Œå¯¼å‡ºç»™`AutoTokenizer`ç”¨ï¼Œå¦‚æœè¦ä¸Šä¼ åˆ°hubè¯·å¿…å¡«
    vocab_size=8021,
    n_positions=1024,
    n_ctx=1024,
    n_embd=768,
    n_layer=6,
    n_head=6,
    pad_token_id=tokenizer.pad_token_id,   # å‰é¢æ„å»ºçš„tokenizerçš„ PAD ID
    task_specific_params={
        "text-generation": {
            "do_sample": True,
            "max_length": 120
        }
    }
)
```

ç„¶åæ„å»ºæ¨¡å‹, ç›´æ¥æŠŠä¸Šé¢å®šä¹‰å¥½çš„ `configs` ä¸¢ç»™ `TFGPT2LMHeadModel` å°±åˆ›å»ºå¥½äº†ã€‚å¦‚æœè¦é€šè¿‡ `Keras` çš„APIè¿›è¡Œè®­ç»ƒçš„è¯ï¼Œéœ€è¦å¯¹æ¨¡å‹è¿›è¡Œcompileä¸€ä¸‹ï¼Œå‰é¢ä¹Ÿæåˆ°lossè¿™é‡Œä¼šæœ‰å‘ã€‚

``` python
from transformers import TFGPT2LMHeadModel

model = TFGPT2LMHeadModel(config)
loss = model.compute_loss
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-08)

model.compile(
    optimizer=optimizer,
    loss=[loss, *[None] * model.config.n_layer],
)
```

## Train

è®­ç»ƒå‰å¯ä»¥è‡ªå®šä¹‰ä¸ªcallbackï¼Œæ¯ä¸ªepochsç»“æŸåä¿å­˜ä¸‹æ¨¡å‹

``` python
class AutoSaveCallback(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        self.model.save_pretrained("path/to/save")

callbacks = [AutoSaveCallback()]

model.fit(
    train_dataset,
    epochs=50,
    steps_per_epoch=2000,
    callbacks=callbacks,
)
```

## ä¸€äº›ä¾‹å­

- ä½ å¯ä»¥åœ¨colabä¸Šå°è¯•æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹: [gpt2_quickly.ipynb](https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/gpt2_quickly.ipynb)
- ä¸€ä¸ªè¿˜åœ¨æµ‹è¯•ä¸­çš„mediuné‡çº§çš„GPT2ä¸­æ–‡æ¨¡å‹: [gpt2_medium_chinese.ipynb](https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/gpt2_medium_chinese.ipynb)
- åŸºäºä¸Šé¢çš„æ¨¡å‹ï¼ŒFinetuneçš„å°è¯´ç”Ÿæˆæ¨¡å‹: [ai_noval_demo.ipynb](https://colab.research.google.com/github/mymusise/gpt2-quickly/blob/main/examples/ai_noval_demo.ipynb)


<div style="text-align:center">
    <img src ="/images/dl/transformer_example/gpt2-medium-chinese-homepage.jpeg" style="width:80%"/>
    <div><a>gpt2_medium_chinese</a></div>
</div>

